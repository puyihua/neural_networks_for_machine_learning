Stochastic gradient descent

 - online
 - mini batches

mini batches:
1. need to be balanced for classes.
2. lecture 6 page 8

tricks for mini-batch:
  1. break the symmetry by initializing the weight to be small random values.
  2. initialize the weight to be proportional to sqrt(fan-in)

  3. shift the inputs —— make it a zero mean over the whole training set
  4. scale the inputs -- make it have a unit variance over the whole training set.
  5. more thorough method : decorrlate the input components(PCA)

  6. momentum: v(t) = alpha * v(t-1) - learningRate * partial(t)
              delta weight = v(t)

    and a better type on momentum : lecture 6 page 21

  7. a separate adaptive learning rate for each connection
        delta Weight(i, j) = - learningRate * g(i, j) * partial(i, j)
        if partial(t)*partial(t-1) <0
        then g = g(t-1) + 0.5
        else g = g(t-1) * 0.95

  8. RMSprop - a mini-batch version of Rprop
    .... lecture 6  page 29



tomorrow on 2017.8.17
READ  <no more pesky learning rate>

 ↑ now on 8.17, not see this paper ,but two lectures a day.


Q:   Hyperbolic tangent v/s logistic sigmoid
The lecture says that the hyperbolic tangent is better than the logistic sigmoid
 in one respect that it produces hidden activations that are roughly zero mean.
 Why can't we achieve the same by using logistic sigmoid hidden units and normalizing
  their outputs (zero mean, unit variance)?


A:
Though not covered in this course, there is a method called batch normalization
where you add a layer that maintains statistics about its inputs and attempts to
normalize them to have zero mean and unit variance. It helps a bunch when used with
logistic activations, but it also often helps when using other activations like tanh
and ReLU.

Since tanh IS (2* sigmoid - 1) I think it is essentially just a linearly transformed
sigmoid already.
